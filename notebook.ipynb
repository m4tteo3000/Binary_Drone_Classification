{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e38eb3eb0766886",
   "metadata": {},
   "source": [
    "<h1><center>Binary Drone Classification<center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ae7f91cfb1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from textwrap import indent\n",
    "from typing import Dict, List, Union, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine Learning\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "import cnn_on_cnn_tuner as kt\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorboard.compat.tensorflow_stub.io.gfile import exists\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bd9babd6e57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import Processor\n",
    "from src.cnn import ClassificationModel\n",
    "from src.svm import SVM, resize\n",
    "from src.grad_cam import GradCAM\n",
    "from src.cnn_on_cnn import TransferCNN\n",
    "from src.svm_on_cnn import FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ece5dbde0e67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "BASE_PATH = Path.cwd()\n",
    "DATA_PATH = BASE_PATH.joinpath('data')\n",
    "MODEL_PATH = BASE_PATH.joinpath('models')\n",
    "\n",
    "# Define constants for reproducibility\n",
    "SEED = 42\n",
    "LOG_LEVEL = 0\n",
    "\n",
    "runtime_dict = {} # For storing runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e494a218c264861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress warnings and set global seed values\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') # suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(LOG_LEVEL)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# make directories if it doesn't exist'\n",
    "MODEL_PATH.mkdir(exist_ok=True) \n",
    "MODEL_PATH.joinpath('history').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2f6bf1d65ef6d",
   "metadata": {},
   "source": [
    "<h3><center>Modules<center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f43d7e62996cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "class Processor:\n",
    "    CLASS_MAP = {'fixed-wing drone': 0, 'multi-rotor drone': 1}\n",
    "\n",
    "    def __init__(self,\n",
    "                 real_img_dir: str,\n",
    "                 real_csv_path: str,\n",
    "                 generated_img_dir: str,\n",
    "                 generated_csv_path: str,\n",
    "                 test_img_dir: str,\n",
    "                 test_csv_path: str,\n",
    "                 size: Tuple[int, int] = (128, 128),\n",
    "                 vis_samples: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the ImageProcessor with directory paths and processing parameters.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.vis_samples = vis_samples\n",
    "        self.data = {k: {'X': [], 'y': []} for k in ['real', 'generated', 'test', 'mixed']}\n",
    "\n",
    "        # Process all datasets\n",
    "        datasets = {\n",
    "            'real': (real_img_dir, real_csv_path),\n",
    "            'generated': (generated_img_dir, generated_csv_path),\n",
    "            'test': (test_img_dir, test_csv_path)\n",
    "        }\n",
    "\n",
    "        for dtype, (img_dir, csv_path) in datasets.items():\n",
    "            self._process_dataset(img_dir, csv_path, dtype)\n",
    "\n",
    "        self._create_mixed_dataset()\n",
    "        self._normalize_and_shuffle_data()\n",
    "\n",
    "    def _process_dataset(self, img_dir: str, csv_path: str, dataset_type: str) -> None:\n",
    "        \"\"\"Process a single dataset of images and their annotations.\"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        img_dir = Path(img_dir)\n",
    "        vis_indices = self._get_visualization_indices(df)\n",
    "        samples_to_visualize = []\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f'Processing {dataset_type} images'):\n",
    "            img_path = img_dir / row['filename']\n",
    "            if not img_path.exists():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                processed_image = self._process_single_image(img_path, row, dataset_type)\n",
    "                if processed_image and idx in vis_indices:\n",
    "                    samples_to_visualize.append(processed_image)\n",
    "\n",
    "                if processed_image:\n",
    "                    self.data[dataset_type]['X'].append(np.array(processed_image))\n",
    "                    self.data[dataset_type]['y'].append(self.CLASS_MAP[row['type']])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path.name}: {e}\")\n",
    "\n",
    "        # Visualize all samples together after processing\n",
    "        if samples_to_visualize:\n",
    "            fig, axes = plt.subplots(1, len(samples_to_visualize), figsize=(6 * len(samples_to_visualize), 6))\n",
    "            if len(samples_to_visualize) == 1:\n",
    "                axes = [axes]\n",
    "\n",
    "            for ax, img in zip(axes, samples_to_visualize):\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(f'{dataset_type.capitalize()} - Processed Image')\n",
    "                ax.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    def _process_single_image(self, img_path: Path, row: pd.Series, dataset_type: str) -> Image.Image:\n",
    "        \"\"\"Process a single image including loading, cropping, and resizing.\"\"\"\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        bbox = self._get_bbox(img, row, dataset_type)\n",
    "        crop = img.crop(bbox)\n",
    "        return self._resize_and_pad(crop)\n",
    "\n",
    "    def _get_bbox(self, img: Image.Image, row: pd.Series, dataset_type: str) -> List[int]:\n",
    "        \"\"\"Calculate bounding box coordinates based on dataset type.\"\"\"\n",
    "        if dataset_type == 'generated':\n",
    "            width, height = img.size\n",
    "            return [int(row[k] * (width if 'x' in k else height))\n",
    "                   for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "        return [row[k] for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "\n",
    "    def _resize_and_pad(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Resize and pad image to target size while maintaining aspect ratio.\"\"\"\n",
    "        img_w, img_h = img.size\n",
    "        scale = min(self.size[0] / img_w, self.size[1] / img_h)\n",
    "        new_size = tuple(int(dim * scale) for dim in (img_w, img_h))\n",
    "\n",
    "        resized = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "        padded = Image.new('RGB', self.size, (0, 0, 0))\n",
    "        paste_pos = tuple((t - n) // 2 for t, n in zip(self.size, new_size))\n",
    "        padded.paste(resized, paste_pos)\n",
    "        return padded\n",
    "\n",
    "    def _get_visualization_indices(self, df: pd.DataFrame) -> List[int]:\n",
    "        \"\"\"Get random indices for visualization samples.\"\"\"\n",
    "        return random.sample(range(len(df)), min(self.vis_samples, len(df))) if len(df) > 0 else []\n",
    "\n",
    "    def _visualize_sample(self, img: Image.Image, dataset_type: str) -> None:\n",
    "        \"\"\"Visualize a processed image sample.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'{dataset_type.capitalize()} - Processed Image')\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def _create_mixed_dataset(self) -> None:\n",
    "        \"\"\"Create a mixed dataset from real and generated images.\"\"\"\n",
    "        X = np.concatenate([self.data['real']['X'], self.data['generated']['X']])\n",
    "        y = np.concatenate([self.data['real']['y'], self.data['generated']['y']])\n",
    "\n",
    "        indices = np.random.permutation(len(X))\n",
    "        self.data['mixed']['X'], self.data['mixed']['y'] = X[indices], y[indices]\n",
    "\n",
    "    def _normalize_and_shuffle_data(self) -> None:\n",
    "        \"\"\"Normalize pixel values and shuffle datasets.\"\"\"\n",
    "        for dtype in self.data:\n",
    "            X = np.array(self.data[dtype]['X'], dtype='float32') / 255.0\n",
    "            y = np.array(self.data[dtype]['y'])\n",
    "\n",
    "            if dtype != 'mixed':\n",
    "                indices = np.random.permutation(len(X))\n",
    "                X, y = X[indices], y[indices]\n",
    "\n",
    "            self.data[dtype]['X'], self.data[dtype]['y'] = X, y\n",
    "\n",
    "    def save_arrays(self, output_dir: str) -> None:\n",
    "        \"\"\"Save processed arrays to disk.\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        for dtype in self.data:\n",
    "            for arr_type in ['X', 'y']:\n",
    "                name = f'{arr_type}_{dtype}.npy'\n",
    "                arr = self.data[dtype][arr_type]\n",
    "                np.save(output_path / name, arr)\n",
    "                print(f\"{name:<16} | Shape: {str(arr.shape):}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab610570f974b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class ModelArchitecture:\n",
    "\n",
    "    @staticmethod\n",
    "    def cnn_01(hp, input_shape, num_classes):\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        common_params = ModelArchitecture._tune_common_hyperparameters(hp, 'cnn_01_')\n",
    "        regularizer = ModelArchitecture._tune_regularizer(hp, 'cnn_01_')\n",
    "\n",
    "        x = inputs\n",
    "\n",
    "        for filters in [32, 64]:\n",
    "            x = Conv2D(filters, (common_params['kernel_size'],) * 2, activation=common_params['activation'],\n",
    "                       kernel_regularizer=regularizer)(x)\n",
    "            x = BatchNormalization()(x) if common_params['use_batch_norm'] else x\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, activation=common_params['activation'], kernel_regularizer=regularizer)(x)\n",
    "        x = Dropout(common_params['dropout_rate'])(x)\n",
    "\n",
    "        outputs = Dense(1, activation='sigmoid', kernel_regularizer=regularizer)(x)\n",
    "\n",
    "        return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    @staticmethod\n",
    "    def cnn_02(hp, input_shape, num_classes):\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        common_params = ModelArchitecture._tune_common_hyperparameters(hp, 'cnn_02_')\n",
    "        regularizer = ModelArchitecture._tune_regularizer(hp, 'cnn_02_')\n",
    "\n",
    "        x = inputs\n",
    "\n",
    "        for filters in [32, 64, 128]:\n",
    "            x = Conv2D(filters, (common_params['kernel_size'],) * 2, activation=common_params['activation'],\n",
    "                       kernel_regularizer=regularizer)(x)\n",
    "            x = BatchNormalization()(x) if common_params['use_batch_norm'] else x\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, activation=common_params['activation'], kernel_regularizer=regularizer)(x)\n",
    "        x = Dropout(common_params['dropout_rate'])(x)\n",
    "\n",
    "        outputs = Dense(1, activation='sigmoid', kernel_regularizer=regularizer)(x)\n",
    "\n",
    "        return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    @staticmethod\n",
    "    def cnn_03(hp, input_shape, num_classes):\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        common_params = ModelArchitecture._tune_common_hyperparameters(hp, 'cnn_03_')\n",
    "        regularizer = ModelArchitecture._tune_regularizer(hp, 'cnn_03_')\n",
    "\n",
    "        x = inputs\n",
    "\n",
    "        for filters in [32, 64, 128, 256]:\n",
    "            x = Conv2D(filters, (common_params['kernel_size'],) * 2, activation=common_params['activation'],\n",
    "                       kernel_regularizer=regularizer)(x)\n",
    "            x = BatchNormalization()(x) if common_params['use_batch_norm'] else x\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, activation=common_params['activation'], kernel_regularizer=regularizer)(x)\n",
    "        x = Dropout(common_params['dropout_rate'])(x)\n",
    "\n",
    "        outputs = Dense(1, activation='sigmoid', kernel_regularizer=regularizer)(x)\n",
    "\n",
    "        return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _tune_common_hyperparameters(hp, prefix):\n",
    "        return {\n",
    "            'activation': hp.Choice(f'{prefix}activation', ['relu', 'elu', 'leaky_relu']),\n",
    "            'kernel_size': hp.Choice(f'{prefix}kernel_size', [3, 5]),\n",
    "            'use_batch_norm': hp.Boolean(f'{prefix}use_batch_norm'),\n",
    "            'dropout_rate': hp.Float(f'{prefix}dropout_rate', 0.20, 0.50, step=0.05)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _tune_regularizer(hp, prefix=''):\n",
    "        return tf.keras.regularizers.l2(hp.Float(f'{prefix}l2_reg', 1e-6, 1e-3, sampling='log'))\n",
    "\n",
    "\n",
    "class HyperModel(kt.HyperModel):\n",
    "\n",
    "    def __init__(self, input_shape, num_classes, architecture_fn):\n",
    "        self.input_shape, self.num_classes, self.architecture_fn = input_shape, num_classes, architecture_fn\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = self.architecture_fn(hp, self.input_shape, self.num_classes)\n",
    "        model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "                      loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "\n",
    "class ClassificationModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: str,\n",
    "                 model_dir: str,\n",
    "                 num_classes: int=2, seed: int=42):\n",
    "        self.data_dir, self.model_dir, self.num_classes = data_dir, model_dir, num_classes\n",
    "        self.models, self.tuners = {}, {}\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "        for dataset in ['generated', 'mixed']:\n",
    "            setattr(self, f'X_{dataset}', np.load(os.path.join(data_dir, f'X_{dataset}.npy')))\n",
    "            setattr(self, f'y_{dataset}', np.load(os.path.join(data_dir, f'y_{dataset}.npy')).astype('float32'))\n",
    "\n",
    "        self.X, self.y = self.X_mixed, self.y_mixed\n",
    "\n",
    "    def set_dataset(self, dataset_type: str):\n",
    "\n",
    "        if dataset_type not in ['generated', 'mixed']:\n",
    "            raise ValueError(\"dataset_type must be one of: 'generated', 'mixed'\")\n",
    "\n",
    "        self.X = getattr(self, f'X_{dataset_type}')\n",
    "        self.y = getattr(self, f'y_{dataset_type}')\n",
    "\n",
    "    def create_tuner(self, name: str, architecture_type: str = 'cnn_01'):\n",
    "\n",
    "        architectures = {f'cnn_0{i}': getattr(ModelArchitecture, f'cnn_0{i}') for i in range(1, 4)}\n",
    "\n",
    "        if architecture_type not in architectures:\n",
    "            raise ValueError(f\"Architecture type must be one of {list(architectures.keys())}\")\n",
    "\n",
    "        self.tuners[name] = kt.RandomSearch(\n",
    "            HyperModel(self.X.shape[1:], self.num_classes, architectures[architecture_type]),\n",
    "            objective='val_accuracy', max_trials=5,\n",
    "            directory=os.path.join(self.model_dir, 'tuning'), project_name=name)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def tune_model(self, name: str, epochs=30, validation_split=0.2):\n",
    "        if name not in self.tuners:\n",
    "            raise ValueError(f\"Tuner '{name}' not found. Create it first with create_tuner()\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.tuners[name].search(\n",
    "            self.X, self.y,\n",
    "            epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "        )\n",
    "\n",
    "        best_hps = self.tuners[name].get_best_hyperparameters(1)[0]\n",
    "\n",
    "        model = self.tuners[name].hypermodel.build(best_hps)\n",
    "\n",
    "        history = model.fit(\n",
    "            self.X, self.y,\n",
    "            epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "\n",
    "        self.models[name] = model\n",
    "\n",
    "        self.save_training_info(name, history.history, runtime)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def save_training_info(self, name: str, history: dict, runtime: float):\n",
    "        info = {\n",
    "            'model_name': name,\n",
    "            'history': history,\n",
    "            'runtime': runtime,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "        history_path = os.path.join(self.model_dir, f'history/{name}_training.json')\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(info, f, indent=4)\n",
    "\n",
    "    def load_training_info(self, name: str) -> dict:\n",
    "        history_path = os.path.join(self.model_dir, f'history/{name}_training.json')\n",
    "        try:\n",
    "            with open(history_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise ValueError(f\"Training history for model '{name}' not found\")\n",
    "\n",
    "    def get_best_hyperparameters(self, name: str):\n",
    "\n",
    "        if name not in self.tuners:\n",
    "            raise ValueError(f\"Tuner '{name}' not found\")\n",
    "\n",
    "        return self.tuners[name].get_best_hyperparameters(1)[0]\n",
    "\n",
    "    def save_model(self, name: str):\n",
    "\n",
    "        if name not in self.models:\n",
    "            raise ValueError(f\"Model '{name}' not found\")\n",
    "\n",
    "        model_path = os.path.join(self.model_dir, f'{name}.keras')\n",
    "        self.models[name].save(model_path)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def load_model(self, name: str):\n",
    "\n",
    "        model_path = os.path.join(self.model_dir, f'{name}.keras')\n",
    "        self.models[name] = tf.keras.models.load_model(model_path)\n",
    "        self.models[name].compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return self.models[name]\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models available. Train or load a model first.\")\n",
    "\n",
    "        return {name: (model.predict(X) > 0.5).astype(int).flatten()\n",
    "                for name, model in self.models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfae56eaa1b472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Optional, Union, List\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from joblib import dump, load\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, model_dir: str, seed: int = 42, is_transfer: bool=False) -> None:\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.is_transfer = is_transfer\n",
    "        self.pipeline = Pipeline([\n",
    "            ('pca', PCA(n_components=0.95, random_state=seed)),\n",
    "            ('svm', SVC(random_state=seed))\n",
    "        ])\n",
    "        self.param_grid = {\n",
    "            'svm__kernel': ['rbf', 'linear'],\n",
    "            'svm__C': [0.1, 1, 10],\n",
    "            'svm__gamma': ['scale', 'auto', 0.01, 0.1]\n",
    "        }\n",
    "        self.best_estimator: Optional[Pipeline] = None\n",
    "        self.history: Optional[Dict] = None\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> Tuple[Pipeline, float]:\n",
    "        start_time = time.time()\n",
    "        if self.is_transfer:\n",
    "            X_train_prepared = X_train.reshape(X_train.shape[0], -1)\n",
    "        else:\n",
    "            resized = resize(X_train, (X_train.shape[0], 64, 64, X_train.shape[3]))\n",
    "            X_train_prepared = resized.reshape(resized.shape[0], -1)\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            self.pipeline,\n",
    "            self.param_grid,\n",
    "            cv=5,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        grid.fit(X_train_prepared, y_train)\n",
    "        self.best_estimator = grid.best_estimator_\n",
    "\n",
    "        self.history = {\n",
    "            'best_params': grid.best_params_,\n",
    "            'cv_results': grid.cv_results_,\n",
    "            'best_score': grid.best_score_\n",
    "        }\n",
    "\n",
    "        end_time = time.time()\n",
    "        running_time = end_time - start_time\n",
    "\n",
    "        print(f\"Best parameters: {grid.best_params_}\")\n",
    "        print(f\"Training completed in {running_time:.2f} seconds\")\n",
    "\n",
    "        return self.best_estimator, running_time\n",
    "\n",
    "    def save(self, name: str) -> None:\n",
    "        \"\"\"Save the trained model.\"\"\"\n",
    "        if self.best_estimator:\n",
    "            dump(self.best_estimator, self.model_dir / f'{name}.joblib')\n",
    "        else:\n",
    "            print(\"No best estimator found. Train the model first.\")\n",
    "\n",
    "    def save_history(self, name: str) -> None:\n",
    "        if self.history:\n",
    "            dump(self.history, self.model_dir / f'history/{name}_history.joblib')\n",
    "        else:\n",
    "            print(\"No training history found. Train the model first.\")\n",
    "\n",
    "    def test(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:\n",
    "        if not self.best_estimator:\n",
    "            raise ValueError(\"No model found. Train or load a model first.\")\n",
    "\n",
    "        resized = resize(X_test, (X_test.shape[0], 64, 64, X_test.shape[3]))\n",
    "        X_test_prepared = resized.reshape(resized.shape[0], -1)\n",
    "\n",
    "        y_pred = self.best_estimator.predict(X_test_prepared)\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def load(self, name: str) -> None:\n",
    "        model_path = self.model_dir / f'{name}.joblib'\n",
    "        if not model_path.exists():\n",
    "            raise FileNotFoundError(f\"No model found at {model_path}\")\n",
    "\n",
    "        self.best_estimator = load(model_path)\n",
    "\n",
    "    def load_history(self, name: str) -> Dict:\n",
    "        history_path = self.model_dir / f'history/{name}_training.joblib'\n",
    "        if not history_path.exists():\n",
    "            raise FileNotFoundError(f\"No history found at {history_path}\")\n",
    "\n",
    "        return load(history_path)\n",
    "\n",
    "\n",
    "def resize(array: np.ndarray, new_shape: Tuple[int, int, int, int]) -> np.ndarray:\n",
    "    batch_size, new_height, new_width, channels = new_shape\n",
    "    resized_array = np.zeros((batch_size, new_height, new_width, channels))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img = array[i]\n",
    "        # Use INTER_AREA for downsampling [[1]](https://stackoverflow.com/questions/48121916/numpy-resize-rescale-image)\n",
    "        resized_array[i] = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return resized_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddabe82dab817a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model_path: Path):\n",
    "        self.cnn = tf.keras.models.load_model(model_path)\n",
    "        self.extractor = tf.keras.Model(\n",
    "            inputs=self.cnn.input,\n",
    "            outputs=self.cnn.layers[-2].output\n",
    "        )\n",
    "\n",
    "    def extract_features(self, data: np.ndarray) -> np.ndarray:\n",
    "        features = self.extractor.predict(data)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8230543a14ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt  # Add this import\n",
    "\n",
    "\n",
    "class HyperModel(kt.HyperModel):\n",
    "    def __init__(self, input_shape, num_classes, base_model, freeze_layers=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.base_model = base_model\n",
    "        self.freeze_layers = freeze_layers\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Create transfer model with hyperparameters\n",
    "        inputs = Input(shape=self.input_shape, name='transfer_input')\n",
    "        x = inputs\n",
    "\n",
    "        base_layers = self.base_model.layers[:-1]\n",
    "\n",
    "        # Add prefix to make layer names unique\n",
    "        for i, layer in enumerate(base_layers):\n",
    "            if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "                continue\n",
    "            if self.freeze_layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "            config = layer.get_config()\n",
    "            config['name'] = f'transfer_{layer.name}_{i}'\n",
    "            new_layer = type(layer).from_config(config)\n",
    "            x = new_layer(x)\n",
    "\n",
    "        # Add final dense layer\n",
    "        outputs = Dense(1, activation='sigmoid', name='transfer_output')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # Compile model with hyperparameters\n",
    "        model.compile(\n",
    "            optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "class TransferCNN:\n",
    "    def __init__(self,\n",
    "                 data_dir: str,\n",
    "                 model_dir: str,\n",
    "                 num_classes=2):\n",
    "        self.data_dir = data_dir\n",
    "        self.model_dir = model_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.base_model = None\n",
    "        self.transfer_model = None\n",
    "        self.training_history = {}\n",
    "        self.training_times = {}\n",
    "\n",
    "        self.X = np.load(os.path.join(data_dir, 'X_real.npy'))\n",
    "        self.y = np.load(os.path.join(data_dir, 'y_real.npy'))\n",
    "        self.y = self.y.astype('float32')\n",
    "\n",
    "    def train(self, epochs=10, validation_split=0.2, batch_size=32, phase='initial'):\n",
    "        if self.transfer_model is None:\n",
    "            raise ValueError(\"Transfer model not created. Call create_transfer_model first.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        history = self.transfer_model.fit(\n",
    "            self.X, self.y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]\n",
    "        )\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Store training time\n",
    "        training_time = end_time - start_time\n",
    "        self.training_times[phase] = training_time\n",
    "\n",
    "        # Store training history\n",
    "        self.training_history[phase] = {\n",
    "            'accuracy': history.history['accuracy'],\n",
    "            'loss': history.history['loss'],\n",
    "            'val_accuracy': history.history['val_accuracy'],\n",
    "            'val_loss': history.history['val_loss']\n",
    "        }\n",
    "\n",
    "        return history\n",
    "\n",
    "    def save_training_info(self, model_name: str):\n",
    "        \"\"\"Save training history and times to JSON files\"\"\"\n",
    "        if not self.training_history or not self.training_times:\n",
    "            print(\"No training information to save.\")\n",
    "            return self\n",
    "\n",
    "        # Save training history\n",
    "        history_path = os.path.join(self.model_dir, f'history/{model_name}_training.json')\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(self.training_history, f, indent=4)\n",
    "        print(f\"Training history saved to '{history_path}'\")\n",
    "\n",
    "        # Save training times\n",
    "        times_path = os.path.join(self.model_dir, f'history/{model_name}_times.json')\n",
    "        with open(times_path, 'w') as f:\n",
    "            json.dump(self.training_times, f, indent=4)\n",
    "        print(f\"Training times saved to '{times_path}'\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def load_base_model(self, model_name: str):\n",
    "        model_path = os.path.join(self.model_dir, f'{model_name}.keras')\n",
    "        self.base_model = tf.keras.models.load_model(model_path)\n",
    "        return self\n",
    "\n",
    "    def create_transfer_model(self, freeze_layers: bool = True):\n",
    "        if self.base_model is None:\n",
    "            raise ValueError(\"Base model not loaded. Call load_base_model first.\")\n",
    "\n",
    "        input_shape = self.X.shape[1:]\n",
    "\n",
    "        base_layers = self.base_model.layers[:-1]\n",
    "\n",
    "        inputs = Input(shape=input_shape, name='transfer_input')\n",
    "        x = inputs\n",
    "\n",
    "        # Add prefix to make layer names unique\n",
    "        for i, layer in enumerate(base_layers):\n",
    "            if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "                continue\n",
    "            if freeze_layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "            # Create a new layer with the same configuration but a unique name\n",
    "            config = layer.get_config()\n",
    "            config['name'] = f'transfer_{layer.name}_{i}'\n",
    "            new_layer = type(layer).from_config(config)\n",
    "            x = new_layer(x)\n",
    "\n",
    "        outputs = Dense(1, activation='sigmoid', name='transfer_output')(x)\n",
    "\n",
    "        self.transfer_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        self.transfer_model.compile(\n",
    "            optimizer=Adam(learning_rate=1e-4),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def unfreeze_layers(self, num_layers: int = 2):\n",
    "\n",
    "        if self.transfer_model is None:\n",
    "            raise ValueError(\"Transfer model not created. Call create_transfer_model first.\")\n",
    "\n",
    "        for layer in self.transfer_model.layers[-num_layers - 1:-1]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        self.transfer_model.compile(\n",
    "            optimizer=Adam(learning_rate=1e-5),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def save_model(self, model_name: str):\n",
    "\n",
    "        if self.transfer_model is None:\n",
    "            raise ValueError(\"No transfer model to save. Create and train a model first.\")\n",
    "\n",
    "        model_path = os.path.join(self.model_dir, f'{model_name}.keras')\n",
    "        self.transfer_model.save(model_path)\n",
    "        print(f\"Model saved to '{model_path}'\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        if self.transfer_model is None:\n",
    "            raise ValueError(\"No transfer model available. Create and train a model first.\")\n",
    "\n",
    "        pred = self.transfer_model.predict(X)\n",
    "        return (pred > 0.5).astype(int).flatten()\n",
    "\n",
    "    def hyperparameter_search(self, epochs=10, validation_split=0.2, max_trials=3):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning using keras-tuner.\n",
    "        \"\"\"\n",
    "        if self.base_model is None:\n",
    "            raise ValueError(\"Base model not loaded. Call load_base_model first.\")\n",
    "\n",
    "        input_shape = self.X.shape[1:]\n",
    "\n",
    "        # Create tuner\n",
    "        hypermodel = HyperModel(\n",
    "            input_shape=input_shape,\n",
    "            num_classes=self.num_classes,\n",
    "            base_model=self.base_model\n",
    "        )\n",
    "\n",
    "        tuner = kt.RandomSearch(\n",
    "            hypermodel,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=max_trials,\n",
    "            directory='models/cnn_on_cnn_tuner',\n",
    "            project_name='transfer_tuning'\n",
    "        )\n",
    "\n",
    "        # Split data for validation\n",
    "        val_size = int(len(self.X) * validation_split)\n",
    "        train_x = self.X[:-val_size]\n",
    "        train_y = self.y[:-val_size]\n",
    "        val_x = self.X[-val_size:]\n",
    "        val_y = self.y[-val_size:]\n",
    "\n",
    "        # Perform the search\n",
    "        tuner.search(\n",
    "            train_x, train_y,\n",
    "            epochs=epochs,\n",
    "            validation_data=(val_x, val_y),\n",
    "            callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "        )\n",
    "\n",
    "        # Get the best hyperparameters\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "        # Build the model with the best hyperparameters\n",
    "        self.transfer_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "        print(\"Best hyperparameters found:\")\n",
    "        print(f\"Learning rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a0a5085d53dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, last_conv_layer_name=None):\n",
    "        self.model = model\n",
    "        self.last_conv_layer_name = last_conv_layer_name\n",
    "        if last_conv_layer_name is None:\n",
    "            self.last_conv_layer_name = self._find_last_conv_layer()\n",
    "\n",
    "    def compute_heatmap(self, img_array, pred_index=None):\n",
    "        if not isinstance(img_array, np.ndarray) or img_array.shape != (128, 128, 3):\n",
    "            raise ValueError(\"Input must be a numpy array with shape (128, 128, 3)\")\n",
    "\n",
    "        grad_model = tf.keras.models.Model(\n",
    "            [self.model.inputs],\n",
    "            [self.model.get_layer(self.last_conv_layer_name).output, self.model.output]\n",
    "        )\n",
    "\n",
    "        # Add batch dimension\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, predictions = grad_model(img_array)\n",
    "            if pred_index is None:\n",
    "                pred_index = tf.argmax(predictions[0])\n",
    "            class_channel = predictions[:, pred_index]\n",
    "\n",
    "        grads = tape.gradient(class_channel, conv_outputs)\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "        conv_outputs = conv_outputs[0]\n",
    "        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "        heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "        return heatmap.numpy()\n",
    "\n",
    "    def display_heatmap(self, img_array, heatmap, alpha=0.4):\n",
    "        # Ensure the input is in the correct format\n",
    "        if not isinstance(img_array, np.ndarray) or img_array.shape != (128, 128, 3):\n",
    "            raise ValueError(\"Input must be a numpy array with shape (128, 128, 3)\")\n",
    "\n",
    "        # Resize heatmap to match input image dimensions\n",
    "        heatmap = cv2.resize(heatmap, (128, 128))\n",
    "        heatmap = np.uint8(255 * heatmap)\n",
    "        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "        # Convert input array to uint8 if needed\n",
    "        if img_array.dtype != np.uint8:\n",
    "            img_array = (img_array * 255).astype(np.uint8)\n",
    "\n",
    "        superimposed_img = heatmap * alpha + img_array\n",
    "        superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Display original image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Display image with heatmap\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Image with Heatmap')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def analyze_image(self, img_array):\n",
    "        heatmap = self.compute_heatmap(img_array)\n",
    "        self.display_heatmap(img_array, heatmap)\n",
    "\n",
    "    def _find_last_conv_layer(self):\n",
    "        for layer in reversed(self.model.layers):\n",
    "            if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.SeparableConv2D)):\n",
    "                return layer.name\n",
    "        raise ValueError(\"Could not find a convolutional layer in the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7102162afe66",
   "metadata": {},
   "source": [
    "<h3><center>Functions<center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74b8480c918c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plots the training history obtained during the training of a machine learning model.\n",
    "\n",
    "    This function visualizes the accuracy and loss values for both the training and validation\n",
    "    phases over the training epochs. Two subplots are created: one displaying training and\n",
    "    validation accuracy, and the other displaying training and validation loss. Each plot\n",
    "    includes annotations for better readability, such as axis labels, titles, legends, and\n",
    "    integer epoch markers.\n",
    "\n",
    "    :param history: A dictionary containing the training history. It must include the keys\n",
    "        'accuracy', 'val_accuracy', 'loss', and 'val_loss', where each value is a list of\n",
    "        values collected for each epoch.\n",
    "    :type history: dict\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    acc = history['accuracy']\n",
    "    val_acc = history['val_accuracy']\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # plotting accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # plotting loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97b081d7566e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names: list):\n",
    "    \"\"\"\n",
    "    Generates and displays a normalized confusion matrix for the given true and predicted\n",
    "    labels, visualized as a heatmap.\n",
    "\n",
    "    The method takes true labels and predicted labels alongside a list of class names\n",
    "    for the heatmap axes, computes the confusion matrix, normalizes it, and displays it\n",
    "    using a heatmap visualization.\n",
    "\n",
    "    :param y_true: True labels. An array or list containing the actual class labels.\n",
    "    :type y_true: iterable\n",
    "    :param y_pred: Predicted labels. An array or list containing the predicted class labels.\n",
    "    :type y_pred: iterable\n",
    "    :param class_names: Class names used for labeling the axes of the heatmap. The order\n",
    "        of names corresponds to the indices in the confusion matrix.\n",
    "    :type class_names: list\n",
    "    :return: None. The function generates a plot but does not return any value.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize the confusion matrix\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d096de6ef8f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates performance metrics including precision, recall, and F1-score\n",
    "    for both positive and negative classes, as well as the macro-averaged F1-score.\n",
    "    Useful for evaluating classification model performance.\n",
    "\n",
    "    :param y_true: The ground truth binary labels. Expected values are 0 or 1.\n",
    "    :param y_pred: The predicted binary labels. Expected values are 0 or 1.\n",
    "    :return: A dictionary containing performance metrics:\n",
    "             - precision_pos: Precision for the positive class (label 1).\n",
    "             - recall_pos: Recall for the positive class (label 1).\n",
    "             - f1_score_pos: F1-score for the positive class (label 1).\n",
    "             - precision_neg: Precision for the negative class (label 0).\n",
    "             - recall_neg: Recall for the negative class (label 0).\n",
    "             - f1_score_neg: F1-score for the negative class (label 0).\n",
    "             - f1_macro: Macro-averaged F1-score for both classes.\n",
    "    \"\"\"\n",
    "    precision_pos = precision_score(y_true, y_pred, pos_label=1)\n",
    "    recall_pos = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1_score_pos = f1_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "    precision_neg = precision_score(y_true, y_pred, pos_label=0)\n",
    "    recall_neg = recall_score(y_true, y_pred, pos_label=0)\n",
    "    f1_score_neg = f1_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return {\n",
    "        'precision_pos': precision_pos,\n",
    "        'recall_pos': recall_pos,\n",
    "        'f1_score_pos': f1_score_pos,\n",
    "        'precision_neg': precision_neg,\n",
    "        'recall_neg': recall_neg,\n",
    "        'f1_score_neg': f1_score_neg,\n",
    "        'f1_macro': f1_macro\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e6abd3ca2abb2",
   "metadata": {},
   "source": [
    "<h2>Exploratory Data Analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ebd16cf81ce7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import annotations\n",
    "real = pd.read_csv(DATA_PATH / 'real.csv')\n",
    "generated = pd.read_csv(DATA_PATH / 'generated.csv')\n",
    "test = pd.read_csv(DATA_PATH / 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6718922c21545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "real.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6fb2bbea5bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c07be06d0c9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc1e3aac351de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307b541af0f080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d41446b28d8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d26197a0e83802",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e493fe13b6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "real.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48bc50ddeb60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492752ad2ce48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count drone type distribution\n",
    "datasets = {'Real': real, 'Generated': generated, 'Test': test}\n",
    "type_counts = {}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    type_counts[name] = dataset['type'].value_counts()\n",
    "\n",
    "type_counts_df = pd.DataFrame(type_counts).fillna(0).astype(int)\n",
    "type_counts_df['Overall'] = type_counts_df.sum(axis=1)\n",
    "type_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e824231a58508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show random drone images\n",
    "def show_random_images(directory, num_images=5):\n",
    "    \"\"\"\n",
    "    Display a random selection of images from a specified directory. The function identifies\n",
    "    image files in the directory, selects a random subset up to the specified number, and\n",
    "    renders them using matplotlib. The images are automatically converted from BGR\n",
    "    to RGB to ensure correct color representation.\n",
    "\n",
    "    :param directory: The directory path containing image files to be displayed.\n",
    "    :type directory: str\n",
    "    :param num_images: The number of random images to display. Defaults to 5.\n",
    "    :type num_images: int\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    all_files = os.listdir(directory)\n",
    "    image_files = [f for f in all_files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "    random_images = random.sample(image_files, min(num_images, len(image_files)))\n",
    "    for image_file in random_images:\n",
    "        image_path = os.path.join(directory, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for matplotlib\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(image)\n",
    "        plt.title(image_file)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837587af00d0434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images(DATA_PATH / 'generated', num_images=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438cb7434394c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images(DATA_PATH / 'real', num_images=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722f12582eb3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images(DATA_PATH / 'test', num_images=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb4b2f9e1e41e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine real images\n",
    "combined_real = pd.concat([real, test], ignore_index=True)\n",
    "combined_real.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695132ebeef0bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution of width, height, and depth\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot width distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(combined_real['width'], bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Width Distribution')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot height distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(combined_real['height'], color='green', alpha=0.7)\n",
    "plt.title('Height Distribution')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot depth distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(combined_real['depth'], color='red', alpha=0.7)\n",
    "plt.title('Depth Distribution')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7c7809596efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show unique values of width, height, and depth in real dataset\n",
    "combined_real['size'] = list(zip(combined_real['width'], combined_real['height'], combined_real['depth']))\n",
    "combined_real['size'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d3e4ee10b1b6f",
   "metadata": {},
   "source": [
    "**Bounding Boxes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238748633e2aef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate center coordinates of bounding boxes\n",
    "combined_real['xcenter'] = (combined_real['xmin'] + combined_real['xmax']) / 2 / combined_real['width']\n",
    "combined_real['ycenter'] = (combined_real['ymin'] + combined_real['ymax']) / 2 / combined_real['height']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot width distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(combined_real['xcenter'], bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Normalized X Center Distribution')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot height distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(combined_real['ycenter'], bins=30, color='green', alpha=0.7)\n",
    "plt.title('Normalized Y Center Distribution')\n",
    "plt.xlabel('Y')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e9478fc916129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bounding box width and height\n",
    "combined_real['bbox_width'] = combined_real['xmax'] - combined_real['xmin']\n",
    "combined_real['bbox_height'] = combined_real['ymax'] - combined_real['ymin']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot width distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(combined_real['bbox_width'], bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Bounding Box Width Distribution')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot height distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(combined_real['bbox_height'], bins=30, color='green', alpha=0.7)\n",
    "plt.title('Bounding Box Height Distribution')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e29f2e546695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalized bounding box position heatmap\n",
    "heatmap, xedges, yedges = np.histogram2d(combined_real['xcenter'], combined_real['ycenter'], bins=[64, 64])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(heatmap.T, cmap='viridis', cbar=True)\n",
    "plt.title('Normalized Bounding Box Position Heatmap')\n",
    "plt.xlabel('x center')\n",
    "plt.ylabel('y center')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4c0d88c5ad0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate center coordinates of bounding boxes\n",
    "generated['xcenter'] = (generated['xmin'] + generated['xmax']) / 2\n",
    "generated['ycenter'] = (generated['ymin'] + generated['ymax']) / 2\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot width distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(generated['xcenter'], bins=30, color='blue', alpha=0.7)\n",
    "plt.title('X Center Distribution')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot height distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(generated['ycenter'], bins=30, color='green', alpha=0.7)\n",
    "plt.title('Y Center Distribution')\n",
    "plt.xlabel('Y')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4063524899221306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bounding box width and height\n",
    "generated['bbox_width'] = (generated['xmax'] - generated['xmin']) * 320\n",
    "generated['bbox_height'] = (generated['ymax'] - generated['ymin']) * 320\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# plot width distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(generated['bbox_width'], bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Bounding Box Width Distribution')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot height distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(generated['bbox_height'], bins=30, color='green', alpha=0.7)\n",
    "plt.title('Bounding Box Height Distribution')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac63cabaaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create normalized bounding box position heatmap for generated dataset\n",
    "heatmap, xedges, yedges = np.histogram2d(generated['xcenter'], generated['ycenter'], bins=[64, 64])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(heatmap.T, cmap='viridis', cbar=True)\n",
    "plt.title('Normalized Bounding Box Position Heatmap')\n",
    "plt.xlabel('x center')\n",
    "plt.ylabel('y center')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542a26eda3be4b2",
   "metadata": {},
   "source": [
    "<h2>Data Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52b4da31b5b480",
   "metadata": {},
   "source": [
    "The processed files are already in  `data.zip`. No need to do this step then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f20e1061ba96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor(\n",
    "    real_img_dir=str(DATA_PATH / 'real'),\n",
    "    real_csv_path=str(DATA_PATH / 'real.csv'),\n",
    "    generated_img_dir=str(DATA_PATH / 'generated'),\n",
    "    generated_csv_path=str(DATA_PATH / 'generated.csv'),\n",
    "    test_img_dir=str(DATA_PATH / 'test'),\n",
    "    test_csv_path=str(DATA_PATH / 'test.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8083ec1ac9a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_arrays(output_dir=str(DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e54d150f3c09e",
   "metadata": {},
   "source": [
    "<h2>Train</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a495f93e5bf8c",
   "metadata": {},
   "source": [
    "<h3>Convolutional Neural Networks<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692911f910274792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize classifier\n",
    "classifier = ClassificationModel(\n",
    "    data_dir=str(DATA_PATH),\n",
    "    model_dir=str(MODEL_PATH),\n",
    "    num_classes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e0ffbbb8ca2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "for architecture in ['cnn_01', 'cnn_02', 'cnn_03']:\n",
    "    for dataset in ['generated', 'mixed']:\n",
    "        model_name = f\"{architecture}_{dataset}\"\n",
    "        classifier.set_dataset(dataset)\n",
    "        classifier.create_tuner(model_name, architecture)\n",
    "        classifier.tune_model(model_name, epochs=30)\n",
    "        classifier.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50410cb3eef1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and accuracy graphs\n",
    "for architecture in ['cnn_01', 'cnn_02', 'cnn_03']:\n",
    "    for dataset in ['generated', 'mixed']:\n",
    "        name = f\"{architecture}_{dataset}\"\n",
    "        with open(f'{str(MODEL_PATH)}/history/{name}_training.json', 'r') as file:\n",
    "            data = json.load(file)\n",
    "        print(f'\\nModel: {name}')\n",
    "        plot_training_history(data['history'])\n",
    "        print(f\"Runtime: {data['runtime']}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c3d6f352aaba2",
   "metadata": {},
   "source": [
    "<h3>Support Vector Machines</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e408fc8b3da669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize data for svm training\n",
    "X_generated = np.load(str(DATA_PATH / 'X_generated.npy'))\n",
    "X_generated_svm = resize(X_generated, (X_generated.shape[0], 64, 64, X_generated.shape[3]))\n",
    "X_generated_svm_pca = X_generated_svm.reshape(X_generated_svm.shape[0], -1)\n",
    "X_mixed = np.load(str(DATA_PATH / 'X_mixed.npy'))\n",
    "X_mixed_svm = resize(X_mixed, (X_mixed.shape[0], 64, 64, X_mixed.shape[3]))\n",
    "X_mixed_svm_pca = X_mixed_svm.reshape(X_mixed_svm.shape[0], -1)\n",
    "print(X_generated_svm_pca.shape)\n",
    "print(X_mixed_svm_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476dd7550b042e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA generated\n",
    "pca = PCA().fit(X_generated_svm_pca)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb80240e572356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA mixed\n",
    "pca = PCA().fit(X_mixed_svm_pca)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54582e6ac4417b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM generated\n",
    "svm_model = SVM(model_dir=MODEL_PATH, seed=SEED)\n",
    "best_model, training_time = svm_model.train(X_generated_svm, np.load(DATA_PATH / 'y_generated.npy'))\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "runtime_dict['SVM1'] = training_time\n",
    "svm_model.save(\"svm_01\")\n",
    "svm_model.save_history(\"svm_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76490fd3a4aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM mixed\n",
    "svm_model = SVM(model_dir=MODEL_PATH, seed=SEED)\n",
    "best_model, training_time = svm_model.train(X_mixed_svm, np.load(DATA_PATH / 'y_mixed.npy'))\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "runtime_dict['SVM3'] = training_time\n",
    "svm_model.save(\"svm_02\")\n",
    "svm_model.save_history(\"svm_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edea14cf4936cb",
   "metadata": {},
   "source": [
    "<h3>Transfer Models</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640b5615636ced9e",
   "metadata": {},
   "source": [
    "<h4>CNN on CNN</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da42e3439c0c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize transfer-model with paths and number of classes\n",
    "transfer = TransferCNN(\n",
    "    data_dir=DATA_PATH,\n",
    "    model_dir=MODEL_PATH,\n",
    "    num_classes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9e0fd692bc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained base model (generated)\n",
    "transfer.load_base_model('cnn_03_generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5b50fc4b852c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute hyperparamter tuning\n",
    "transfer.hyperparameter_search(\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    max_trials=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90a94f65236423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train new layer\n",
    "history = transfer.train(\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    phase='initial'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df852fa924b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un-freez two layers\n",
    "transfer.unfreeze_layers(num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52cf121a4ca120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune new and unfrozen layers\n",
    "history = transfer.train(\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    phase='fine_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd7343d6571497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and training history\n",
    "transfer.save_model('cnn_on_cnn')\n",
    "transfer.save_training_info('cnn_on_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472e782cd055b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training history\n",
    "with open(f'{str(MODEL_PATH)}/history/cnn_on_cnn_training.json', 'r') as file: # load training history\n",
    "    data = json.load(file)\n",
    "# loss and accuracy graphs\n",
    "for step in ['initial', 'fine_tuning']:\n",
    "    print(f'\\nModel: {step}')\n",
    "    plot_training_history(data[step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91494511f91dd042",
   "metadata": {},
   "source": [
    "<h4>SVM on CNN</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e69c9ea4d6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from base CNN and load real dataset\n",
    "extractor = FeatureExtractor(str(MODEL_PATH / 'cnn_03_generated.keras'))\n",
    "extracted_features = extractor.extract_features(np.load(DATA_PATH / 'X_real.npy'))\n",
    "y_real = np.load(DATA_PATH / 'y_real.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e935f749d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA().fit(extracted_features.reshape(extracted_features.shape[0], -1))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef432331e8f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train SVM on CNN features\n",
    "svm_model = SVM(model_dir=MODEL_PATH, seed=SEED, is_transfer=True)\n",
    "best_model, training_time = svm_model.train(extracted_features, y_real)\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "runtime_dict['SVM2'] = training_time\n",
    "svm_model.save(\"svm_on_cnn\")\n",
    "svm_model.save_history(\"svm_on_cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37630bfe232b77",
   "metadata": {},
   "source": [
    "<h2>Test</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc53b0e45575849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "CNN1 = load_model(str(MODEL_PATH / 'cnn_03_generated.keras'))\n",
    "CNN2 = load_model(str(MODEL_PATH / 'cnn_on_cnn.keras'))\n",
    "CNN3 = load_model(str(MODEL_PATH / 'cnn_02_mixed.keras'))\n",
    "SVM1 = joblib.load(MODEL_PATH / 'svm_01.joblib')\n",
    "SVM2 = joblib.load(MODEL_PATH / 'svm_on_cnn.joblib')\n",
    "SVM3 = joblib.load(MODEL_PATH / 'svm_02.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a6a98a06ca7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "X_test = np.load(DATA_PATH / 'X_test.npy')\n",
    "y_test = np.load(DATA_PATH / 'y_test.npy')\n",
    "X_resized = resize(X_test, (X_test.shape[0], 64, 64, X_test.shape[3]))\n",
    "X_resized = X_resized.reshape(X_resized.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c329e0e7579b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {} # initialize results dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23458a81951aa524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test performance of the CNNs\n",
    "for name, model in {'CNN1': CNN1, 'CNN2': CNN2, 'CNN3': CNN3}.items():\n",
    "    start_time = time.time()\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"{name}: {duration:.10f} seconds\")\n",
    "    results[name] = get_performance_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7a9bee877f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test standard SVM performance\n",
    "for name, model in {'SVM1': SVM1, 'SVM3': SVM3}.items():\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_resized)\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"{name}: {duration:.10f} seconds\")\n",
    "    results[name] = get_performance_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de74550436a2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test SVM on CNN performance\n",
    "extractor = FeatureExtractor(str(MODEL_PATH / 'cnn_03_generated.keras'))\n",
    "start_time = time.time()\n",
    "extracted_features = extractor.extract_features(X_test)\n",
    "y_pred = SVM2.predict(extracted_features)\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"SVM2: {duration:.10f} seconds\")\n",
    "results['SVM2'] = get_performance_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d07dffc05390bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(results).T\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a6027fe8192f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get running time for standard CNNs\n",
    "for name, file in {'CNN1': 'cnn_03_generated_training.json', 'CNN3': 'cnn_02_mixed_training.json',}.items():\n",
    "    with open(f'{str(MODEL_PATH)}/history/{file}', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    runtime_dict[name] = data.get('runtime', data.get('history', {}).get('runtime', 0))\n",
    "# get running time for CNN on CNN\n",
    "with open(f'{str(MODEL_PATH)}/history/cnn_on_cnn_times.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "# add running times to runtime dict\n",
    "runtime_dict['CNN2'] = data['initial'] + data['fine_tuning'] + runtime_dict['CNN1']\n",
    "runtime_dict['SVM2'] = runtime_dict['SVM2'] + runtime_dict['CNN1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb648ccec70bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f497ac15b3e8f0e",
   "metadata": {},
   "source": [
    "<h3>Grad-CAM and Error Analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255fde89f92a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for applying Grad-CAM\n",
    "def display_grad_cam_images(model, num_img: int=3):\n",
    "    \"\"\"\n",
    "    Displays Grad-CAM images for the given model and test dataset, based on the\n",
    "    highest and lowest logits from the prediction. This function identifies\n",
    "    images with extreme prediction values (both high and low), computes their\n",
    "    Grad-CAM visualizations, and outputs them alongside the corresponding\n",
    "    logit value.\n",
    "\n",
    "    :param model: The trained model used for prediction and Grad-CAM analysis.\n",
    "    :type model: Any\n",
    "    :param num_img: The number of images to extract for high and low logits.\n",
    "                    Defaults to 3.\n",
    "    :type num_img: int, optional\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    high_logit = []\n",
    "    img_array = np.load(str(DATA_PATH / 'X_test.npy'))\n",
    "    predictions = model.predict(img_array)\n",
    "    high_logit.extend(np.argsort(predictions.flatten())[-num_img:].tolist())\n",
    "    high_logit.extend(np.argsort(predictions.flatten())[:num_img].tolist())\n",
    "    # loop through images with high and low logits\n",
    "    for idx in high_logit:\n",
    "        img = img_array[idx]\n",
    "        grad_cam = GradCAM(model)\n",
    "        grad_cam.analyze_image(img)\n",
    "        print(f'Logit Val: {predictions[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9dcbe771ef3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for applying Grad-CAM to mis-classfied images\n",
    "def display_misclassified(model):\n",
    "    \"\"\"\n",
    "    Displays a randomly selected misclassified example from a given model's predictions.\n",
    "    This function evaluates the model's predictions against the actual values and\n",
    "    chooses one misclassified instance randomly. It utilizes GradCAM for visualizing\n",
    "    the feature importance of the model on the selected example.\n",
    "\n",
    "    :param model: The trained model used for making predictions and GradCAM analysis.\n",
    "    :type model: Any suitable machine learning model with a `.predict` method.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    img_array = np.load(str(DATA_PATH / 'X_test.npy'))\n",
    "    predictions = model.predict(img_array)\n",
    "    misclassified_indices = np.where((predictions > 0.5).astype(int).flatten() != np.load(str(DATA_PATH / 'y_test.npy')))[0]\n",
    "    random_index = random.choice(misclassified_indices)\n",
    "    grad_cam = GradCAM(model)\n",
    "    grad_cam.analyze_image(img_array[random_index])\n",
    "    print(f'Logit Val: {predictions[random_index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf5e3174d6da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_grad_cam_images(CNN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b085db14674ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_grad_cam_images(CNN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee86b01d7fd130",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_grad_cam_images(CNN3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758de57ee5a3621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_misclassified(CNN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175779ca7deb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_misclassified(CNN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcafcaee2fb6347",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_misclassified(CNN3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede6fcb36ffe07",
   "metadata": {},
   "source": [
    "<h3>Model Configurations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c9d5fe7a68305",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_config = CNN1.get_config()\n",
    "print(json.dumps(cnn1_config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f34d91ec2503b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2_config = CNN2.get_config()\n",
    "print(json.dumps(cnn2_config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4d6fff267ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3_config = CNN3.get_config()\n",
    "print(json.dumps(cnn3_config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863611052f1f5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1_params = SVM1.get_params()\n",
    "print(svm1_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f082d42e7779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm2_params = SVM2.get_params()\n",
    "print(svm2_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a8b4de5cf86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm3_params = SVM3.get_params()\n",
    "print(svm3_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe65699-af64-4462-a209-cd094ccf864c",
   "metadata": {},
   "source": [
    "**DISCLAIMER**\n",
    "\n",
    "Data provided is already preprocessed!\n",
    "\n",
    "Below you can find the preprocessing process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab5a4431601999",
   "metadata": {},
   "source": [
    "**Filtering**\n",
    "```Python\n",
    "#This Script removes duplicates and RGB outliers from the real_drones Dataset\n",
    "\n",
    "import numpy as np\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# ---- CONFIGURATION ----\n",
    "src_dir      = \"real_drones\"               # ← this is your “source” folder\n",
    "dup_dir      = \"real_drones_duplicates\"    # where perceptual‐duplicate files get moved\n",
    "outlier_dir  = \"real_drones_RGB_outliers\"  # where RGB‐outlier files get moved\n",
    "\n",
    "# ---- 1) REMOVE DUPLICATES VIA PERCEPTUAL HASH ----\n",
    "import imagehash\n",
    "\n",
    "hash_map = {}  # phash -> first filename\n",
    "phash_threshold = 0  # Hamming distance threshold (0 = exact phash match)\n",
    "for fname in os.listdir(src_dir):\n",
    "    if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "    path = os.path.join(src_dir, fname)\n",
    "    with Image.open(path) as img:\n",
    "        ph = imagehash.phash(img)\n",
    "    # check against existing hashes\n",
    "    duplicate_found = False\n",
    "    for existing_ph, original_fname in list(hash_map.items()):\n",
    "        if ph - existing_ph <= phash_threshold:\n",
    "            # duplicate detected\n",
    "            shutil.move(path, os.path.join(dup_dir, fname))\n",
    "            print(f\"Moved perceptual duplicate {fname} (matches {original_fname}) to {dup_dir}\")\n",
    "            duplicate_found = True\n",
    "            break\n",
    "    if not duplicate_found:\n",
    "        hash_map[ph] = fname\n",
    "\n",
    "# ---- RGB THRESHOLD FILTER (mean <5 or >250) ----\n",
    "for fname in os.listdir(src_dir):\n",
    "    if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "    path = os.path.join(src_dir, fname)\n",
    "    with Image.open(path) as img:\n",
    "        arr = np.array(img)\n",
    "        if arr.ndim == 3 and arr.shape[2] >= 3:\n",
    "            # compute mean for R, G, B\n",
    "            mean_rgb = arr[:, :, :3].reshape(-1, 3).mean(axis=0)\n",
    "            if (mean_rgb < 5).any() or (mean_rgb > 250).any():\n",
    "                shutil.move(path, os.path.join(outlier_dir, fname))\n",
    "                print(f\"Moved RGB threshold outlier {fname} to {outlier_dir}\")\n",
    "        else:\n",
    "            print(f\"Skipping non-RGB image {fname}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e743e26fdb7a4b",
   "metadata": {},
   "source": [
    "**Augmentation**\n",
    "```Python\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load annotations\n",
    "df = pd.read_csv('annotations_real.csv', sep=';')\n",
    "\n",
    "# Albumentations transforms\n",
    "night_transform = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.4, -0.3), contrast_limit=(-0.1, 0.5), p=1.0),\n",
    "    A.RGBShift(r_shift_limit=0, g_shift_limit=0, b_shift_limit=(-40, -20), p=1.0)\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "clahe_transform = A.Compose([\n",
    "    A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=1.0)\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "# Collect new rows\n",
    "augmented_rows = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    img_path = os.path.join('real_drones', row['filename'])\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]\n",
    "    label = row['uav']\n",
    "\n",
    "    for transform, suffix in [(night_transform, '_night'), (clahe_transform, '_clahe')]:\n",
    "        result = transform(image=img, bboxes=[bbox], class_labels=[label])\n",
    "        new_img = cv2.cvtColor(result['image'], cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        new_filename = row['filename'].replace('.jpg', f'{suffix}.jpg')\n",
    "        cv2.imwrite(os.path.join('real_drones', new_filename), new_img)\n",
    "\n",
    "        new_bbox = list(map(int, result['bboxes'][0]))\n",
    "        new_row = row.copy()\n",
    "        new_row['filename'] = new_filename\n",
    "        new_row['xmin'], new_row['ymin'], new_row['xmax'], new_row['ymax'] = new_bbox\n",
    "        augmented_rows.append(new_row)\n",
    "\n",
    "# Append to CSV\n",
    "df_aug = pd.DataFrame(augmented_rows)\n",
    "df_final = pd.concat([df, df_aug], ignore_index=True)\n",
    "df_final.to_csv('annotations_real.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Augmentation complete and CSV updated.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43001db04bb71b17",
   "metadata": {},
   "source": [
    "**Blender Script**\n",
    "```Python\n",
    "import bpy\n",
    "import random\n",
    "import numpy as np\n",
    "import mathutils\n",
    "from pathlib import Path\n",
    "from bpy_extras.object_utils import world_to_camera_view\n",
    "import csv\n",
    "\n",
    "\n",
    "class DatasetGenerator:\n",
    "    PATHS = {\n",
    "        'images_dir': 'images',\n",
    "        'annotations_file': 'annotations.csv',\n",
    "        'hdri_dir': 'hdris'\n",
    "    }\n",
    "\n",
    "    RENDER_SETTINGS = {\n",
    "        'format': 'PNG',\n",
    "        'resolution': 320\n",
    "    }\n",
    "\n",
    "    CSV_HEADERS = [\n",
    "        'filename', 'drone_type',\n",
    "        'drone_position_x', 'drone_position_y', 'drone_position_z',\n",
    "        'camera_rotation_x', 'camera_rotation_y', 'camera_rotation_z',\n",
    "        'bbox_min_x', 'bbox_min_y', 'bbox_width', 'bbox_height',\n",
    "        'hdri_name'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, output_path: str, scene_setter):\n",
    "        self.output_path = Path(output_path)\n",
    "        self.scene_setter = scene_setter\n",
    "        self.image_dir = self.output_path / self.PATHS['images_dir']\n",
    "        self.csv_path = self.output_path / self.PATHS['annotations_file']\n",
    "        self.hdri_dir = self.output_path / self.PATHS['hdri_dir']\n",
    "        self.image_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.render = bpy.context.scene.render\n",
    "        self.render.image_settings.file_format = self.RENDER_SETTINGS['format']\n",
    "        self.render.resolution_x = self.render.resolution_y = self.RENDER_SETTINGS['resolution']\n",
    "\n",
    "        self.hdri_files = self._load_hdri_files()\n",
    "\n",
    "    def _load_hdri_files(self):\n",
    "        \"\"\"Load all HDR files from the specified directory\"\"\"\n",
    "        hdri_files = list(self.hdri_dir.glob('*.hdr'))\n",
    "        hdri_files.extend(self.hdri_dir.glob('*.hdri'))\n",
    "        hdri_files.extend(self.hdri_dir.glob('*.exr'))\n",
    "\n",
    "        if not hdri_files:\n",
    "            raise ValueError(f\"No HDR/HDRI/EXR files found in {self.hdri_dir}\")\n",
    "\n",
    "        return hdri_files\n",
    "\n",
    "    def generate_dataset(self, num_images: int):\n",
    "        if not self.hdri_files:\n",
    "            raise ValueError(\"No HDRI files available for generation\")\n",
    "\n",
    "        with open(self.csv_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.CSV_HEADERS)\n",
    "\n",
    "            for i in range(num_images):\n",
    "                hdri_path = str(random.choice(self.hdri_files))\n",
    "                if not self.scene_setter.set_random_scene(hdri_path):\n",
    "                    continue\n",
    "\n",
    "                drone = next((obj for obj in bpy.data.objects\n",
    "                              if not obj.hide_viewport and any(d in obj.name\n",
    "                                                               for d in self.scene_setter.drones)), None)\n",
    "                if not drone:\n",
    "                    continue\n",
    "\n",
    "                bpy.context.view_layer.update()\n",
    "                bbox = self.scene_setter.calculate_object_screen_coverage(drone)\n",
    "                if not bbox:\n",
    "                    continue\n",
    "\n",
    "                filename = f\"img_{i:04d}.png\"\n",
    "                self.render.filepath = str(self.image_dir / filename)\n",
    "                bpy.ops.render.render(write_still=True)\n",
    "\n",
    "                writer.writerow([\n",
    "                    filename, drone.name, *drone.location,\n",
    "                    *self.scene_setter.camera.rotation_euler,\n",
    "                    *bbox, Path(hdri_path).stem\n",
    "                ])\n",
    "\n",
    "\n",
    "class SetScene:\n",
    "    DOME = {\n",
    "        'outer_radius': 5.0,\n",
    "        'inner_radius': 4.0,\n",
    "        'min_height': 1\n",
    "    }\n",
    "\n",
    "    CAMERA_SETTINGS = {\n",
    "        'position': (0, 0, 0),\n",
    "        'track_axis': '-Z',\n",
    "        'up_axis': 'Y'\n",
    "    }\n",
    "\n",
    "    DRONE_ROTATION = {\n",
    "        'angle_limit': 25,\n",
    "        'full_rotation': 360\n",
    "    }\n",
    "\n",
    "    VISIBILITY = {\n",
    "        'min_area': 0.01,\n",
    "        'max_attempts': 20\n",
    "    }\n",
    "\n",
    "    def __init__(self, drones: list = None):\n",
    "        self.drones = drones or []\n",
    "        self.scene = bpy.context.scene\n",
    "        self.camera = self.scene.camera\n",
    "        self.camera.location = mathutils.Vector(self.CAMERA_SETTINGS['position'])\n",
    "\n",
    "    def set_random_scene(self, hdri_path: str):\n",
    "        drone_name = random.choice(self.drones)\n",
    "        drone = bpy.data.objects.get(drone_name)\n",
    "        if not drone:\n",
    "            return False\n",
    "\n",
    "        self._setup_scene_for_drone(drone_name)\n",
    "        self.setup_hdri_background(hdri_path)\n",
    "        return self.position_drone(drone)\n",
    "\n",
    "    def setup_hdri_background(self, path):\n",
    "        self.scene.world.use_nodes = True\n",
    "        nodes = self.scene.world.node_tree.nodes\n",
    "        links = self.scene.world.node_tree.links\n",
    "        nodes.clear()\n",
    "\n",
    "        env_tex = nodes.new('ShaderNodeTexEnvironment')\n",
    "        env_tex.image = bpy.data.images.load(str(path), check_existing=True)\n",
    "        bg_node = nodes.new('ShaderNodeBackground')\n",
    "        output_node = nodes.new('ShaderNodeOutputWorld')\n",
    "\n",
    "        links.new(env_tex.outputs['Color'], bg_node.inputs['Color'])\n",
    "        links.new(bg_node.outputs['Background'], output_node.inputs['Surface'])\n",
    "\n",
    "    def position_drone(self, obj):\n",
    "        for _ in range(self.VISIBILITY['max_attempts']):\n",
    "            coords = self._generate_dome_coordinates(self.DOME)\n",
    "            obj.location = mathutils.Vector(coords)\n",
    "            obj.rotation_euler = mathutils.Euler((\n",
    "                np.radians(random.uniform(-self.DRONE_ROTATION['angle_limit'],\n",
    "                                          self.DRONE_ROTATION['angle_limit'])),\n",
    "                np.radians(random.uniform(-self.DRONE_ROTATION['angle_limit'],\n",
    "                                          self.DRONE_ROTATION['angle_limit'])),\n",
    "                np.radians(random.uniform(0, self.DRONE_ROTATION['full_rotation']))\n",
    "            ))\n",
    "\n",
    "            direction = obj.location - self.camera.location\n",
    "            self.camera.rotation_euler = direction.to_track_quat(\n",
    "                self.CAMERA_SETTINGS['track_axis'],\n",
    "                self.CAMERA_SETTINGS['up_axis']\n",
    "            ).to_euler()\n",
    "\n",
    "            if self._is_drone_visible(obj):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def calculate_object_screen_coverage(self, obj):\n",
    "        depsgraph = bpy.context.evaluated_depsgraph_get()\n",
    "        corners = [obj.matrix_world @ mathutils.Vector(corner)\n",
    "                   for corner in obj.evaluated_get(depsgraph).bound_box]\n",
    "\n",
    "        coords_2d = []\n",
    "        for corner in corners:\n",
    "            coord = world_to_camera_view(self.scene, self.camera, corner)\n",
    "            coords_2d.append((coord.x, 1.0 - coord.y))\n",
    "\n",
    "        if not coords_2d:\n",
    "            return None\n",
    "\n",
    "        x_coords, y_coords = zip(*coords_2d)\n",
    "        min_x, max_x = min(x_coords), max(x_coords)\n",
    "        min_y, max_y = min(y_coords), max(y_coords)\n",
    "\n",
    "        min_x = max(0.0, min(min_x, 1.0))\n",
    "        max_x = max(0.0, min(max_x, 1.0))\n",
    "        min_y = max(0.0, min(min_y, 1.0))\n",
    "        max_y = max(0.0, min(max_y, 1.0))\n",
    "\n",
    "        width = max_x - min_x\n",
    "        height = max_y - min_y\n",
    "\n",
    "        if width <= 0 or height <= 0:\n",
    "            return None\n",
    "\n",
    "        return (min_x, min_y, width, height)\n",
    "\n",
    "    def _setup_scene_for_drone(self, active_drone_name):\n",
    "        for name in self.drones:\n",
    "            if obj := bpy.data.objects.get(name):\n",
    "                obj.hide_render = obj.hide_viewport = (name != active_drone_name)\n",
    "\n",
    "    def _is_drone_visible(self, obj):\n",
    "        bpy.context.view_layer.update()\n",
    "        if bbox := self.calculate_object_screen_coverage(obj):\n",
    "            return bbox[2] * bbox[3] >= self.VISIBILITY['min_area']\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_dome_coordinates(params):\n",
    "        phi = random.uniform(0, 2 * np.pi)\n",
    "        theta = random.uniform(0, np.pi / 2)\n",
    "        r = random.uniform(params['inner_radius'], params['outer_radius'])\n",
    "\n",
    "        return (r * np.sin(theta) * np.cos(phi),\n",
    "                r * np.sin(theta) * np.sin(phi),\n",
    "                max(r * np.cos(theta), params['min_height']))\n",
    "\n",
    "\n",
    "# Usage example\n",
    "scene_setter = SetScene(['Fixed_01', 'Fixed_02', 'Rotor_01', 'Rotor_02'])\n",
    "dataset_generator = DatasetGenerator(\n",
    "    output_path='/Users/oskardale/Desktop/DroneImages',\n",
    "    scene_setter=scene_setter\n",
    ")\n",
    "dataset_generator.generate_dataset(10)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
